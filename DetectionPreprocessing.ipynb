{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import math\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from datetime import date\n",
    "from statistics import mean, median\n",
    "from time import sleep\n",
    "from datetime import datetime, timedelta\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import gmean\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import emoji\n",
    "import torch\n",
    "from slovnet.model.emb import NavecEmbedding\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from navec import Navec\n",
    "path = 'navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
    "navec = Navec.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.read_excel(f\"{151563780}_new_wave_1.xlsx\")\n",
    "start = pd.DataFrame(columns=start.columns)\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f2af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [91198168,100752412,\n",
    "          113148910,119906733,\n",
    "          138216345,139140790,\n",
    "          139823386,143874380,\n",
    "          146523913,150048947,\n",
    "          150606036,151973186]:\n",
    "    basic = pd.read_excel(f\"df_total_{i}.xlsx\")\n",
    "    start = start.append(basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea9dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [9943236,22003393,\n",
    "          8601593,75212617,\n",
    "          75640333,86283646,\n",
    "          93199503,100339707,\n",
    "          100817270,103487056,\n",
    "          109266137,110259188,\n",
    "          125406299,125436671,\n",
    "          148056502,150136196,\n",
    "          151563780,154532809]:\n",
    "    basic = pd.read_excel(f\"{i}_new_wave_1.xlsx\")\n",
    "    start = start.append(basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "start['new_merge'] = 0\n",
    "for i in tqdm(start['Артикул'].unique()):\n",
    "    for j in start[start['Артикул']==i]['Отзыв_x'].unique():\n",
    "        for k in start[(start['Артикул']==i) & (start['Отзыв_x']==j)]['id комментатора_x'].unique():\n",
    "            count+=1\n",
    "            start.loc[(start['Артикул']==i) &\n",
    "                      (start['Отзыв_x']==j) &\n",
    "                      (start['id комментатора_x']==k),'new_merge'] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "start['id_for_merge'] = start['id комментатора_x'].astype(str)+start['new_merge'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e838ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = start.dropna()\n",
    "start = start.drop_duplicates()\n",
    "start.head()\n",
    "clear = start.groupby(['id_for_merge'])['Отзыв_y'].count().reset_index()\n",
    "clear = clear.rename(columns={'Отзыв_y':'count_reviews'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f33992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(start['Артикул']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c15fe",
   "metadata": {},
   "source": [
    "# cleaning + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223a2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mon = {'января':1,\n",
    "       'февраля':2,\n",
    "       'марта':3,\n",
    "       'апреля':4,\n",
    "       'мая':5,\n",
    "       'июня':6,\n",
    "       'июля':7,\n",
    "       'августа':8,\n",
    "       'сентября':9,\n",
    "       'октября':10,\n",
    "       'ноября':11,\n",
    "       'декабря':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a307d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = emoji.replace_emoji(text, replace='')# удаление эмодзи\n",
    "    text = re.sub(r'\\d+', '', text) # удаление цифр\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # удаление лишних пробелов\n",
    "    tokens = word_tokenize(text) # Токенизация\n",
    "    tokens = [token.lower() for token in tokens] # Приведение к нижнему регистру\n",
    "    lemmatizer = WordNetLemmatizer() # Лемматизация\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation + '’') # Очистка от знаков пунктуации и других символов\n",
    "    tokens = [token.translate(table) for token in tokens]\n",
    "    tokens = [token for token in tokens if len(token) > 0] # Удаление пустых токенов\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def numbers(text):\n",
    "    return int(re.findall(r'\\d+', text)[0])\n",
    "def correct_date(element):\n",
    "    element = element.split(' ')\n",
    "    no_comma = [rep.replace(',', '') for rep in element] \n",
    "    if len(no_comma) == 3:\n",
    "        no_comma = no_comma[:2] + ['2023'] + no_comma[2:]\n",
    "        \n",
    "    corr_month = [*map(mon.get, no_comma, no_comma)]\n",
    "    date = datetime(int(corr_month[2]),corr_month[1],\n",
    "                    int(corr_month[0]),\n",
    "                    int(corr_month[3][:2]),\n",
    "                    int(corr_month[3][3:]))\n",
    "    return date\n",
    "def on_site(text):\n",
    "    if ((text.find('месяца') >= 0) or \n",
    "        (text.find('месяцев') >= 0) or\n",
    "        (text.find('месяц') >=0 )):\n",
    "        numb = int(re.findall(r'\\d+', text)[0])\n",
    "    else:\n",
    "        numb = int(re.findall(r'\\d+', text)[0])*12\n",
    "    return numb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f79965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_func1_df(new_func1):\n",
    "    new_func1['id'] = new_func1['id_for_merge'].apply(numbers)\n",
    "    new_func1['id_srt'] = new_func1['Артикул'].astype(str) + '_' + new_func1['id'].astype(str)\n",
    "    count = 0\n",
    "    new_func1['sorted'] = 0\n",
    "    for i in new_func1['id_srt'].unique():\n",
    "        count+=1\n",
    "        new_func1.loc[new_func1['id_srt']==i,'sorted'] = count\n",
    "\n",
    "    new_func1['main_id'] = new_func1['Артикул'].astype(str) + '_' + new_func1['sorted'].astype(str)\n",
    "\n",
    "    new_func1 = new_func1.rename(columns={\n",
    "        'Отзыв_x':'main_review',\n",
    "        'Отзыв_y':'other_review',\n",
    "        'Оценка отзыва':'main_rate',\n",
    "        'Оценки по отзывам':'other_rate',\n",
    "        'id комментатора_x':'client_name',\n",
    "        'Количество отзывов комментатора':'count_reviews',\n",
    "        'Количество фотографий':'count_photo',\n",
    "        'Заголовок':'item',\n",
    "        'Количество отзывов':'main_item_reviews_count',\n",
    "        'Артикул':'headphones_id'\n",
    "    })\n",
    "    \n",
    "    new_func1['count_reviews'] = new_func1['count_reviews'].apply(numbers)\n",
    "    new_func1['main_date'] = new_func1['Дата отзыва'].apply(correct_date)\n",
    "    new_func1['other_date'] = new_func1['Дата отзыва комментатора'].apply(correct_date)\n",
    "    new_func1['other_review_len'] = new_func1['other_review'].str.len()\n",
    "    new_func1['main_review_len'] = new_func1['main_review'].str.len()\n",
    "    new_func1['month_on_site'] = new_func1['Количество лет на сайте'].apply(on_site)\n",
    "    new_func1['main_review_list'] = new_func1['main_review'].apply(preprocess_text)\n",
    "    new_func1['other_review_list'] = new_func1['other_review'].apply(preprocess_text)\n",
    "    new_func1['item_list'] = new_func1['item'].apply(preprocess_text)\n",
    "    new_func1 = new_func1.sort_values(['sorted','other_date'],ascending=[True,False])\n",
    "    \n",
    "    sort_new_func1 = new_func1.copy()\n",
    "    sort_new_func1 = sort_new_func1.reset_index(drop=True)\n",
    "    sort_new_func1 = sort_new_func1[['headphones_id',\n",
    "        'main_item_reviews_count', 'sorted','main_id','client_name','month_on_site','count_reviews',\n",
    "        \n",
    "        'main_review','main_review_list','main_date','main_review_len','main_rate',\n",
    "        \n",
    "        'item','item_list',\n",
    "        \n",
    "        'other_review','other_review_list','other_date','other_review_len','other_rate','count_photo'\n",
    "    ]]\n",
    "    return sort_new_func1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc48041",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_func1 = start.copy()\n",
    "sort_new_func1 = new_func1_df(new_func1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2f370",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clear = sort_new_func1.groupby(['main_id'])['other_review'].count().reset_index()\n",
    "clear = clear.rename(columns={'other_review':'count_reviews'})\n",
    "sort_new_func1 = sort_new_func1.drop(columns='count_reviews')\n",
    "sort_new_func1 = sort_new_func1.merge(clear,on='main_id',how='left')\n",
    "sort_new_func1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c20a7",
   "metadata": {},
   "source": [
    "# признак близости отзывов внутри клиента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc27030",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_func2 = sort_new_func1.copy()\n",
    "new_func2['date_median'] = 0\n",
    "new_func2['date_std'] = 0\n",
    "new_func2['rates_neg_share'] = 0\n",
    "cosine_navec = {}\n",
    "\n",
    "for i in tqdm(new_func2['main_id'].unique()):\n",
    "    text_embd = {}\n",
    "    gpt_embd = {}\n",
    "    min_diff_revies = []\n",
    "    rates = new_func2.loc[new_func2['main_id']==i]['other_rate'].value_counts().reset_index().copy()\n",
    "    new_func2.loc[new_func2['main_id']==i,'rates_neg_share'] = 1-rates[(rates['index']==5) |\n",
    "                                (rates['index']==4)]['other_rate'].sum()/rates['other_rate'].sum()\n",
    "    for j in new_func2.loc[new_func2['main_id']==i].index:\n",
    "        text_embd_list = []\n",
    "        for k in new_func2.loc[(new_func2.index==j)]['other_review_list'][j]:\n",
    "            try: \n",
    "                text_embd_list.append(navec[k])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        if len(text_embd_list) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            mean_arr = np.mean(np.array(text_embd_list), axis=0)    \n",
    "            text_embd.update({j: mean_arr})\n",
    "            if (abs((text_embd[j]+3).sum())>=0.1e-10) == False:\n",
    "                text_embd.pop(j)\n",
    "            \n",
    "        if (new_func2.loc[new_func2['main_id']==i].index.to_list().index(j) !=\n",
    "           (len(new_func2.loc[new_func2['main_id']==i].index)-1)):\n",
    "            min_diff_revies.append((new_func2.loc[(new_func2['main_id']==i) &\n",
    "                                                 (new_func2.index==j)]['other_date'][j] - \n",
    "                                    new_func2.loc[(new_func2['main_id']==i) &\n",
    "                                                 (new_func2.index==j+1)]['other_date'][j+1]).total_seconds()/60)\n",
    "            new_func2.loc[new_func2['main_id']==i,'date_median'] = int(np.median(min_diff_revies))\n",
    "            new_func2.loc[new_func2['main_id']==i,'date_std'] = int(np.std(min_diff_revies))\n",
    "    \n",
    "    embeddings_list = list(text_embd.values())    \n",
    "    if len(embeddings_list) !=0:\n",
    "        matrix = cosine_similarity(embeddings_list)\n",
    "        values = matrix[np.triu_indices(matrix.shape[0], k=1)]\n",
    "        cosine_navec.update({i: values})\n",
    "    else:\n",
    "        cosine_navec.update({i: [0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddef278",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = [val for sublist in cosine_navec.values() for val in sublist]\n",
    "\n",
    "plt.figure(figsize=[12,7])\n",
    "plt.hist(val, bins=50)\n",
    "plt.title('Распределение косинусных близостей отзывов')\n",
    "plt.xlabel('Значение косинусной близости')\n",
    "plt.ylabel('Количество повторений')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame.from_dict(cosine_navec, orient='index')\n",
    "\n",
    "val = [val for sublist in cosine_navec.values() for val in sublist]\n",
    "q70 = np.quantile(val, 0.85)\n",
    "print(q70)\n",
    "\n",
    "for i in cosine_navec.keys():\n",
    "    dd.loc[dd.index==i,'q70_ratio'] = np.mean(cosine_navec[i]>q70)\n",
    "    dd.loc[dd.index==i,'std'] = np.std(cosine_navec[i])\n",
    "\n",
    "dd = dd[['q70_ratio','std']]\n",
    "dd = dd.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa57f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(val),np.median(val),np.quantile(val,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ff3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_func3 = new_func2.merge(dd,left_on='main_id',right_index=True,how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0058f9e1",
   "metadata": {},
   "source": [
    "# признак повторямости товара среди клиентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def items_repeats(id_item_df):\n",
    "    repeat = id_item_df['item'].value_counts().reset_index()\n",
    "    repeat.head()\n",
    "    item_embeddings = {}\n",
    "    sim = {}\n",
    "    for item in repeat['index']:\n",
    "        item_name_prp = preprocess_text(item)\n",
    "        word_embd = []\n",
    "        for word in item_name_prp:\n",
    "            try: \n",
    "                word_embd.append(navec[word])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        if len(word_embd) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            item_embeddings[item] = np.mean(np.array(word_embd), axis=0)\n",
    "            if (abs((item_embeddings[item]+3).sum())>=0.1e-10) == False:\n",
    "                item_embeddings.pop(item)\n",
    "        \n",
    "    items = list(item_embeddings.keys())\n",
    "    for i in items:\n",
    "        if item_embeddings[i].shape!=(300,):\n",
    "            items.remove(i)\n",
    "    repeat_cl = repeat[repeat['index'].isin(items)]\n",
    "    for _, row in repeat_cl.iterrows():\n",
    "        similar = []\n",
    "        purchased_item = row['index']\n",
    "        pur_item = item_embeddings[purchased_item]\n",
    "        other_items = [item_embeddings[item] for item in items]\n",
    "        similarities = cosine_similarity(pur_item.reshape(1, -1),\n",
    "                                         other_items)[0]\n",
    "        for i, _ in enumerate(similarities): \n",
    "            if similarities[i] > 0.9:\n",
    "                similar.append(i)\n",
    "        sim[purchased_item] = repeat_cl[repeat_cl.index.isin(similar)]['item'].sum()\n",
    "    sim_df = pd.DataFrame([sim]).T.reset_index()\n",
    "    \n",
    "    count_items = repeat.merge(sim_df,how='left',on='index')\n",
    "    count_items = count_items.rename(columns={\n",
    "        'item':'value_c',\n",
    "        'index':'item',\n",
    "        0:'value_c_navec'\n",
    "    })\n",
    "    return count_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f1d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_count_df = pd.DataFrame(columns=['headphones_id','item','value_c','value_c_navec'])\n",
    "for i in tqdm(set(new_func3['headphones_id'])):\n",
    "    df_tr = items_repeats(new_func3[new_func3['headphones_id']==i])\n",
    "    df_tr['headphones_id'] = i\n",
    "    item_count_df = item_count_df.append(df_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0662e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_count_df = item_count_df.drop_duplicates()\n",
    "item_count_df = item_count_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c9cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_count_df_corr = item_count_df.copy()\n",
    "item_count_df_corr = item_count_df_corr.fillna(item_count_df_corr['value_c'])\n",
    "item_count_df_corr['corr_counts'] = np.NaN\n",
    "item_count_df_corr['item_mark'] = np.NaN\n",
    "for i in tqdm(set(new_func3['headphones_id'])):\n",
    "    item_count_df_corr.loc[(item_count_df_corr['headphones_id']==i)\n",
    "                     & (item_count_df_corr['value_c']>\n",
    "    item_count_df_corr[item_count_df_corr['headphones_id']==i]['value_c_navec'].max()*0.1),'corr_counts'] = item_count_df_corr[item_count_df_corr['headphones_id']==i]['value_c_navec'].max()\n",
    "    \n",
    "    item_count_df_corr.loc[(item_count_df_corr['headphones_id']==i)\n",
    "                     & (item_count_df_corr['value_c']>\n",
    "    item_count_df_corr[item_count_df_corr['headphones_id']==i]['value_c_navec'].max()*0.1),'item_mark'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab982be",
   "metadata": {},
   "outputs": [],
   "source": [
    "chechh = item_count_df_corr[item_count_df_corr['item_mark']==1].copy()\n",
    "chechh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ffed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chechh['item_list'] = chechh['item'].apply(preprocess_text)\n",
    "ch = chechh.copy()\n",
    "ch['delete'] = 0\n",
    "for i in chechh.index:\n",
    "    if ('микронаушник' in ch.loc[ch.index==i]['item_list'][i] or\n",
    "        'микронаушники' in ch.loc[ch.index==i]['item_list'][i] or\n",
    "        'наушники' in ch.loc[ch.index==i]['item_list'][i] or\n",
    "        'наушник' in ch.loc[ch.index==i]['item_list'][i] or \n",
    "        'стереонаушники' in ch.loc[ch.index==i]['item_list'][i]):\n",
    "        ch.loc[ch.index==i,'delete'] = 1\n",
    "\n",
    "ch.loc[ch['delete']==0,'corr_counts'] = ch['value_c']\n",
    "correct = ch[ch['delete']==0][['headphones_id','item','corr_counts']]\n",
    "\n",
    "item_count_df_corr = item_count_df_corr.merge(correct,how='left',on=['headphones_id','item'])\n",
    "item_count_df_corr['corr_counts_y'].fillna(item_count_df_corr['corr_counts_x'],inplace=True)\n",
    "item_count_df_corr['corr_counts_y'].fillna(item_count_df_corr['value_c'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c62092",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_repeat = []\n",
    "maxes = []\n",
    "for i in item_count_df_corr['headphones_id'].unique():\n",
    "    max_value_item = item_count_df_corr[item_count_df_corr['headphones_id']==i]['corr_counts_y'].max()\n",
    "    print(i,max_value_item)\n",
    "    item_count_df_corr.loc[item_count_df_corr.index==5004,'corr_counts_y']=498\n",
    "    item_count_df_corr.loc[item_count_df_corr.index==32878,'corr_counts_y']=344\n",
    "    item_count_df_corr.loc[item_count_df_corr.index==44569,'corr_counts_y']=291 \n",
    "        \n",
    "    values_repeat.append(list(item_count_df_corr[(item_count_df_corr['headphones_id']==i) &\n",
    "                             (item_count_df_corr['corr_counts_y']!=max_value_item)]['corr_counts_y']))\n",
    "    maxes.append(list(item_count_df_corr[(item_count_df_corr['headphones_id']==i) &\n",
    "                             (item_count_df_corr['corr_counts_y']==max_value_item)]['item']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513882cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = []\n",
    "for sublist in values_repeat:\n",
    "    flat_list.extend(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd436fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_join_item = item_count_df_corr[['headphones_id','item','corr_counts_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1379fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_func4 = new_func3.merge(to_join_item,how='left',on=['item','headphones_id'])\n",
    "new_func4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f955b81",
   "metadata": {},
   "source": [
    "# признак схожести основных отзывов между собой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e3748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_cos_sim(id_item_df):\n",
    "    math = id_item_df[['main_id','main_review_list']]\n",
    "    math = math.drop_duplicates('main_id').reset_index(drop=True)\n",
    "    \n",
    "    embeddings_dict = {}\n",
    "    cosine_main = {}\n",
    "    for i in math['main_id']:\n",
    "        main_embd_list = []\n",
    "        for j in list(math[math['main_id']==i]['main_review_list'])[0]:\n",
    "            try:   \n",
    "                main_embd_list.append(navec[j])\n",
    "            except KeyError:\n",
    "                continue\n",
    "         \n",
    "        if len(main_embd_list) == 0:\n",
    "                pass\n",
    "        else:\n",
    "            main_mean_arr = np.mean(np.array(main_embd_list), axis=0)    \n",
    "            embeddings_dict.update({i: main_mean_arr})\n",
    "            if (abs((embeddings_dict[i]+3).sum())>=0.1e-10) == False:\n",
    "                print(i,math[math['main_id']==i]['main_review_list'],embeddings_dict[i])\n",
    "                embeddings_dict.pop(i)\n",
    "                \n",
    "    cos_sim_dict = {}\n",
    "    cos_q70 = {}\n",
    "    \n",
    "    for key in embeddings_dict.keys():\n",
    "        embedding = embeddings_dict[key]\n",
    "        \n",
    "        cos_sim_list = []\n",
    "        for other_key in embeddings_dict.keys():\n",
    "            if other_key != key:\n",
    "                other_embedding = embeddings_dict[other_key]\n",
    "                cos_sim = cosine_similarity([embedding, other_embedding])\n",
    "                main_values = cos_sim[np.triu_indices(cos_sim.shape[0], k=1)]\n",
    "                cos_sim_list.append(main_values)\n",
    "        \n",
    "        mean_cos_sim = np.median(cos_sim_list)\n",
    "        \n",
    "        cos_sim_dict[key] = mean_cos_sim\n",
    "        cos_q70[key] = round(np.mean(cos_sim_list>q70),2)\n",
    "    return cos_q70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfcc265",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_func5 = pd.DataFrame(columns=new_func4.columns)\n",
    "new_func5['main_simillar'] = 0\n",
    "for i in tqdm(set(new_func4['headphones_id'])):\n",
    "    new = new_func4.loc[new_func4['headphones_id']==i].copy()\n",
    "    new['main_simillar'] = new['main_id'].map(main_cos_sim(new))\n",
    "    new_func5 = new_func5.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_func5 = new_func5.sort_values(by=['sorted','other_date'],ascending=[True,False])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a6342",
   "metadata": {},
   "source": [
    "# признаки с фотографиями и длиной отзывов остальных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_int = list(new_func5.dtypes[new_func5.dtypes==object].index)\n",
    "cols_to_int = [col for col in cols_to_int if col not in ['main_review','other_review','main_id',\n",
    "                                                         'item','main_review_list',\n",
    "                                                         'other_review_list','item_list','client_name']]\n",
    "new_func6 = new_func5.copy()\n",
    "for i in cols_to_int:\n",
    "    new_func6[i] = new_func6[i].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c95a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_func6 = new_func6.drop(columns=['main_review_list','item_list','other_review_list'])\n",
    "new_func6 = new_func6.drop_duplicates()\n",
    "new_func6 = new_func6.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dd5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_df = new_func6.copy()\n",
    "grouped = gpt_df.groupby(by=['headphones_id','main_id','sorted','month_on_site',\n",
    "                'count_reviews',\n",
    "                'main_review',\n",
    "                'main_review_len',\n",
    "                'main_rate',\n",
    "                'date_median',\n",
    "                'date_std',\n",
    "                'rates_neg_share',\n",
    "                'main_simillar',\n",
    "                'q70_ratio','std'\n",
    "                            ])\n",
    "med = grouped['other_review_len','count_photo'].median()\n",
    "med = med.rename(columns={'other_review_len':'med_other_review_len',\n",
    "                     'count_photo':'med_count_photo'})\n",
    "\n",
    "var = grouped['other_review_len','other_rate'].var()\n",
    "var = var.rename(columns={'other_review_len':'var_other_review_len',\n",
    "                     'other_rate':'var_other_rate'})\n",
    "\n",
    "result = pd.concat([med, var], axis=1)\n",
    "result = result.reset_index()\n",
    "result = result.iloc[result['main_id'].drop_duplicates(keep='first').index]\n",
    "result = result.reset_index(drop=True)\n",
    "result = result.sort_values('sorted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4989f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_c_navec = new_func4[['main_id','corr_counts_y']]\n",
    "corr = pd.DataFrame(columns=value_c_navec.columns)\n",
    "for i in tqdm(value_c_navec['main_id'].unique()):\n",
    "    promo_df = value_c_navec[value_c_navec['main_id']==i]\n",
    "    corr = corr.append(promo_df[promo_df['corr_counts_y']!=promo_df['corr_counts_y'].max()])\n",
    "corr = corr[corr['corr_counts_y']!=1].groupby(['main_id'])['corr_counts_y'].agg(['count','max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ccafc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_func7 = result.merge(corr,on='main_id',how='left')\n",
    "new_func7 = new_func7.fillna(0)\n",
    "new_func7['count'] = new_func7['count']/(new_func7['count_reviews']-1)\n",
    "new_func7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8615546",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sort_negg = sort_negg.rename(columns={\n",
    "        'month_on_site':'months_on_site',\n",
    "        'count_reviews':'reviews_number',\n",
    "        'main_review':'review',\n",
    "        'main_rate':'rating',\n",
    "        'date_median':'minutes_between_reviews',\n",
    "        'rates_neg_share':'share_of_negative_ratings',\n",
    "        'q70_ratio':'similarity_of_client_reviews',\n",
    "        'med_other_review_len':'median_length_of_other_reviews',\n",
    "        'med_count_photo':'median_number_of_photos',\n",
    "        'max':'max_similarity_of_products',\n",
    "        'count':'share_similarity_of_products',\n",
    "        'main_simillar':'similarity_of_main_reviews',\n",
    "        'std':'std_similarity_of_client_reviews'})\n",
    "col_to_check = ['months_on_site','headphones_id','reviews_number','minutes_between_reviews',\n",
    "                                  'date_std','share_of_negative_ratings',\n",
    "                                  'similarity_of_client_reviews','median_length_of_other_reviews',\n",
    "                                  'median_number_of_photos','var_other_review_len',\n",
    "                                  'var_other_rate','share_similarity_of_products',\n",
    "                                  'max_similarity_of_products']\n",
    "final = sort_negg[col_to_check].drop_duplicates()\n",
    "final_df = sort_negg.loc[final.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834404e1",
   "metadata": {},
   "source": [
    "# подготовка тестового датафрейма для gpt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded51b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_df = final_df.copy()\n",
    "for i in ['share_of_negative_ratings',\n",
    "          'similarity_of_client_reviews',\n",
    "          'std_similarity_of_client_reviews',\n",
    "          'var_other_rate',\n",
    "          'share_similarity_of_products']:\n",
    "    gpt3_df[i] = gpt3_df[i].round(2)\n",
    "    \n",
    "for i in ['headphones_id',\n",
    "          'sorted',\n",
    "          'months_on_site',\n",
    "          'reviews_number',\n",
    "          'main_review_len',\n",
    "          'rating',\n",
    "          'minutes_between_reviews',\n",
    "          'date_std',\n",
    "          'median_length_of_other_reviews',\n",
    "          'median_number_of_photos',\n",
    "          'var_other_review_len',\n",
    "          'max_similarity_of_products']:\n",
    "    gpt3_df[i] = gpt3_df[i].astype(int)\n",
    "gpt3_df = gpt3_df.sort_values(by=['share_similarity_of_products',\n",
    "                                  'similarity_of_client_reviews',\n",
    "                                  'similarity_of_main_reviews',\n",
    "                                  'minutes_between_reviews'], ascending=[False, False,False,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e974eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "checc = gpt3_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checc.to_excel('final_df_to_save.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877e3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(index_start,index_stop,fake,true):\n",
    "    other = checc.sorted.to_list()\n",
    "    for item in other:\n",
    "        if item in fake:\n",
    "            other.remove(item)\n",
    "    \n",
    "    prompt_message = f'Given client data: {checc.columns.to_list()[1:]}. Determine if a review is fake or not, based on the review and customer data. Label the data.\\n' \n",
    "    \n",
    "    prompt_message += '''Example: \\n'''  \n",
    "    prompt_message += 'Inputs: \\n'\n",
    "    \n",
    "    for i in fake: \n",
    "        prompt_message += f'''data {i}: {list(checc[checc.sorted==i][list(checc.columns)[1:]].values[0])} \\n'''\n",
    "    for i in true:\n",
    "        prompt_message += f'''data {i}: {list(checc[checc.sorted==i][list(checc.columns)[1:]].values[0])} \\n'''\n",
    "    \n",
    "    prompt_message += '\\nOutputs: \\n'\n",
    "    \n",
    "    for i in fake:\n",
    "        prompt_message += f'label {i}: Fake \\n'\n",
    "    for i in true:\n",
    "        prompt_message += f'label {i}: True \\n'\n",
    "        \n",
    "    prompt_message += '\\nInputs: \\n'\n",
    "    \n",
    "    for i in other[index_start:index_stop]:\n",
    "        prompt_message += f'''data {i}: {list(checc[checc.sorted==i][list(checc.columns)[1:]].values[0])} \\n'''\n",
    "        \n",
    "    prompt_message += '\\nOutputs: \\n'\n",
    "    \n",
    "    return prompt_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = [949,3819,2044]\n",
    "true = [3506,4230,3533]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = 'hiden_due_privacy'\n",
    "model_engine = \"text-davinci-003\" \n",
    "temp = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724365e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in tqdm(list(range(3468, 3600))[::4]):\n",
    "    i = j - 4\n",
    "    if i < 0:\n",
    "        #break\n",
    "    print(i, j)\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            engine=model_engine,\n",
    "            prompt=prompt(i,j,fake,true),\n",
    "            temperature=temp,\n",
    "            max_tokens=50)\n",
    "\n",
    "        resp = response.choices[0].text.split('\\n')\n",
    "        data = [tuple(x.strip().split(\": \")) for x in resp]\n",
    "        promo_df = pd.DataFrame(data,columns=['label', 'value'])\n",
    "        promo_df['label'] = promo_df['label'].str.replace('label ', '').astype(int)\n",
    "        labeled_data = labeled_data.append(promo_df)\n",
    "    except:\n",
    "        try:\n",
    "            print('WARNING_lvl1')\n",
    "            response = openai.Completion.create(\n",
    "                engine=model_engine,\n",
    "                prompt=prompt(i,j,fake[:-1],true[:-1]),\n",
    "                temperature=temp,\n",
    "                max_tokens=50)\n",
    "    \n",
    "            resp = response.choices[0].text.split('\\n')\n",
    "            data = [tuple(x.strip().split(\": \")) for x in resp]\n",
    "            promo_df = pd.DataFrame(data,columns=['label', 'value'])\n",
    "            promo_df['label'] = promo_df['label'].str.replace('label ', '').astype(int)\n",
    "            labeled_data = labeled_data.append(promo_df)\n",
    "        except:\n",
    "            print('WARNING_lvl2')\n",
    "            response = openai.Completion.create(\n",
    "                engine=model_engine,\n",
    "                prompt=prompt(i,j,fake[:-2],true[:-2]),\n",
    "                temperature=temp,\n",
    "                max_tokens=50)\n",
    "\n",
    "            resp = response.choices[0].text.split('\\n')\n",
    "            data = [tuple(x.strip().split(\": \")) for x in resp]\n",
    "            promo_df = pd.DataFrame(data,columns=['label', 'value'])\n",
    "            promo_df['label'] = promo_df['label'].str.replace('label ', '').astype(int)\n",
    "            labeled_data = labeled_data.append(promo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d5d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeled_data.to_excel('label_final.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0cd596",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labeled_data['value'].value_counts()/labeled_data['value'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b3909",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = labeled_data.drop_duplicates(subset='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee96124",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gpt3_df[gpt3_df['sorted'].isin(labeled_data[labeled_data['value']=='Fake']['label'])].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d6db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_df[gpt3_df['sorted'].isin(labeled_data[labeled_data['value']=='True']['label'])].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_try_df = gpt3_df.copy()\n",
    "labeled_try_df = labeled_try_df.merge(labeled_data,on='main_id',how='left')\n",
    "drop = labeled_try_df[['months_on_site','reviews_number',\n",
    "              'review','minutes_between_reviews',\n",
    "              'similarity_of_main_reviews','max_similarity_of_products']].duplicated()\n",
    "labeled_try_df = labeled_try_df[~drop]\n",
    "labeled_try_df.to_excel('detection_df.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
